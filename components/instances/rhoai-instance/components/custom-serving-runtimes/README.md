# Custom Serving Runtimes Component

Ce composant ajoute des runtimes de service custom √† OpenShift AI 2.22 suivant la documentation officielle Red Hat et le pattern BU "une feature = un r√©pertoire".

## Runtimes Inclus

### üöÄ NVIDIA Triton Inference Server
**Source**: Chapitre 2.11.23 de la documentation OpenShift AI 2.22

**Frameworks support√©s**:
- TensorFlow (v1 & v2)
- PyTorch (v1)
- ONNX (v1)
- TensorRT (v8)
- Python backend pour mod√®les custom

**Caract√©ristiques**:
- Multi-model serving
- Support GPU et CPU
- Metrics Prometheus int√©gr√©es
- Optimis√© pour l'inf√©rence haute performance
- Backend Python pour mod√®les personnalis√©s

**Configuration**:
```yaml
apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  name: triton-runtime
spec:
  supportedModelFormats:
  - name: tensorflow
  - name: pytorch
  - name: onnx
  - name: tensorrt
  - name: python
```

### üß† Seldon MLServer
**Source**: Chapitre 2.11.3 de la documentation OpenShift AI 2.22

**Frameworks support√©s**:
- Scikit-learn (v1)
- XGBoost (v1)
- LightGBM (v3)
- MLflow (v1)
- Hugging Face (v1)
- Tempo custom models (v1)

**Caract√©ristiques**:
- Multi-model serving
- Support Python ML ecosystem
- Metrics Prometheus int√©gr√©es
- Optimis√© pour mod√®les Python
- Interface V2 protocol compatible

**Configuration**:
```yaml
apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  name: seldon-mlserver-runtime
spec:
  supportedModelFormats:
  - name: sklearn
  - name: xgboost
  - name: lightgbm
  - name: mlflow
  - name: huggingface
```

## Structure du Composant

```
custom-serving-runtimes/
‚îú‚îÄ‚îÄ kustomization.yaml           # Configuration Kustomize
‚îú‚îÄ‚îÄ triton-runtime.yaml          # NVIDIA Triton runtime
‚îú‚îÄ‚îÄ seldon-mlserver-runtime.yaml # Seldon MLServer runtime
‚îî‚îÄ‚îÄ README.md                    # Cette documentation
```

## D√©ploiement GitOps

Ces runtimes sont automatiquement d√©ploy√©s avec l'instance RHOAI via GitOps :

```yaml
# Dans components/instances/rhoai-instance/kustomization.yaml
components:
  - components/model-registry/
  - components/custom-serving-runtimes/  # ‚ú® Nouveau composant
```

## Utilisation

Une fois d√©ploy√©s, ces runtimes apparaissent dans l'interface OpenShift AI :

1. **Model Serving** ‚Üí **Deploy model**
2. S√©lectionner le runtime appropri√© :
   - **Triton** pour TensorFlow, PyTorch, ONNX, TensorRT
   - **Seldon MLServer** pour scikit-learn, XGBoost, MLflow

## Monitoring et Observabilit√©

Les deux runtimes exposent des m√©triques Prometheus :
- **Triton** : port 8002, path `/metrics`
- **Seldon** : port 8080, path `/metrics`

## S√©curit√© et Resources

### Limits par d√©faut
- **CPU** : 2 cores max, 500m requests
- **M√©moire** : 4Gi max, 1Gi requests

### Health Checks
- **Liveness** : `/v2/health/live` (30s initial, 30s period)
- **Readiness** : `/v2/health/ready` (10s initial, 10s period)

## Troubleshooting

### V√©rifier les runtimes
```bash
oc get servingruntime -n redhat-ods-applications
```

### Logs des runtimes
```bash
# Pour un InferenceService utilisant Triton
oc logs -n <namespace> deployment/<inference-service>-predictor -c triton

# Pour un InferenceService utilisant Seldon
oc logs -n <namespace> deployment/<inference-service>-predictor -c mlserver
```

### Status des runtimes
```bash
oc describe servingruntime triton-runtime -n redhat-ods-applications
oc describe servingruntime seldon-mlserver-runtime -n redhat-ods-applications
```

## Compatibilit√©

- **OpenShift AI** : 2.22+
- **KServe** : API v1alpha1
- **OpenShift** : 4.14+
- **Service Mesh** : Compatible Istio

## R√©f√©rences

- [Documentation OpenShift AI 2.22 - Chapitre 2.11.23 Triton](https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/2.22/html/serving_models/)
- [Documentation OpenShift AI 2.22 - Chapitre 2.11.3 Seldon](https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/2.22/html/serving_models/)
- [NVIDIA Triton Inference Server](https://github.com/triton-inference-server/server)
- [Seldon MLServer](https://mlserver.readthedocs.io/)
