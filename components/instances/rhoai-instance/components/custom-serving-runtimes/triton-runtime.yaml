# NVIDIA Triton Inference Server Runtime
# Bas√© sur le chapitre 2.11.23 de la documentation OpenShift AI 2.22
# Supporte TensorFlow, PyTorch, ONNX, TensorRT et autres frameworks
apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  name: triton-runtime
  namespace: redhat-ods-applications
  labels:
    app.kubernetes.io/name: triton-runtime
    app.kubernetes.io/component: serving-runtime
    app.kubernetes.io/part-of: openshift-ai
    runtime: triton
    opendatahub.io/dashboard: "true"
  annotations:
    opendatahub.io/recommended-accelerators: '["nvidia.com/gpu"]'
    opendatahub.io/runtime-version: v23.10
    openshift.io/display-name: "NVIDIA Triton Inference Server"
spec:
  annotations:
    prometheus.kserve.io/path: "/metrics"
    prometheus.kserve.io/port: "8002"
    serving.kserve.io/enable-prometheus-scraping: "true"
    serving.kserve.io/enable-route: "true"
  multiModel: true
  supportedModelFormats:
  # TensorFlow models
  - name: tensorflow
    version: "1"
    autoSelect: true
  - name: tensorflow
    version: "2"
    autoSelect: true
  # PyTorch models  
  - name: pytorch
    version: "1"
    autoSelect: true
  # ONNX models
  - name: onnx
    version: "1"
    autoSelect: true
  # TensorRT models
  - name: tensorrt
    version: "8"
    autoSelect: true
  # Python backend for custom models
  - name: python
    version: "1"
    autoSelect: true
  containers:
  - name: triton
    image: nvcr.io/nvidia/tritonserver:23.10-py3
    command:
    - /bin/bash
    - -c
    args:
    - |
      tritonserver \
        --model-repository=/mnt/models \
        --model-control-mode=explicit \
        --strict-model-config=false \
        --strict-readiness=false \
        --allow-http=true \
        --allow-grpc=true \
        --allow-metrics=true \
        --allow-gpu-metrics=true \
        --http-port=8080 \
        --grpc-port=8081 \
        --metrics-port=8002 \
        --http-thread-count=8 \
        --grpc-max-threads=8 \
        --log-verbose=1 \
        --disable-auto-complete-config
    ports:
    - containerPort: 8080
      name: http
      protocol: TCP
    - containerPort: 8081
      name: grpc
      protocol: TCP
    - containerPort: 8002
      name: metrics
      protocol: TCP
    env:
    - name: STORAGE_URI
      value: "/mnt/models"
    resources:
      requests:
        cpu: 500m
        memory: 1Gi
      limits:
        cpu: "2"
        memory: 4Gi
    volumeMounts:
    - name: model-dir
      mountPath: /mnt/models
    livenessProbe:
      httpGet:
        path: /v2/health/live
        port: http
      initialDelaySeconds: 30
      periodSeconds: 30
      timeoutSeconds: 10
    readinessProbe:
      httpGet:
        path: /v2/health/ready
        port: http
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 5
  volumes:
  - name: model-dir
    emptyDir: {}
  builtInAdapter:
    serverType: triton
    runtimeManagementPort: 8001
    memBufferBytes: 134217728
    modelLoadingTimeoutMillis: 90000
