apiVersion: template.openshift.io/v1
kind: Template
metadata:
  annotations:
    description: "NVIDIA Triton Inference Server - Multi-framework serving runtime for TensorFlow, PyTorch, ONNX, and TensorRT models"
    opendatahub.io/apiProtocol: "REST"
    opendatahub.io/modelServingSupport: '["multi"]'
    openshift.io/display-name: "NVIDIA Triton Inference Server"
    openshift.io/provider-display-name: "NVIDIA Corporation"
    tags: "rhods,rhoai,kserve,servingruntime,triton,tensorflow,pytorch,onnx,tensorrt"
    template.openshift.io/documentation-url: "https://docs.nvidia.com/deeplearning/triton-inference-server/"
    template.openshift.io/long-description: "This template defines resources needed to deploy NVIDIA Triton Inference Server with KServe in Red Hat OpenShift AI for serving TensorFlow, PyTorch, ONNX, and TensorRT models."
  labels:
    opendatahub.io/dashboard: "true"
  name: nvidia-triton-runtime-template
  namespace: redhat-ods-applications
objects:
- apiVersion: serving.kserve.io/v1alpha1
  kind: ServingRuntime
  metadata:
    annotations:
      opendatahub.io/recommended-accelerators: '["nvidia.com/gpu"]'
      opendatahub.io/runtime-version: "23.10"
      openshift.io/display-name: "NVIDIA Triton Inference Server"
    labels:
      opendatahub.io/dashboard: "true"
    name: nvidia-triton-runtime
  spec:
    annotations:
      prometheus.kserve.io/path: "/metrics"
      prometheus.kserve.io/port: "8002"
      serving.kserve.io/enable-prometheus-scraping: "true"
      serving.kserve.io/enable-route: "true"
    builtInAdapter:
      memBufferBytes: 134217728
      modelLoadingTimeoutMillis: 90000
      runtimeManagementPort: 8001
      serverType: triton
    containers:
    - name: triton
      image: nvcr.io/nvidia/tritonserver:23.10-py3
      command:
      - /bin/bash
      - -c
      args:
      - |
        tritonserver \
          --model-repository=/mnt/models \
          --model-control-mode=explicit \
          --strict-model-config=false \
          --strict-readiness=false \
          --allow-http=true \
          --allow-grpc=true \
          --allow-metrics=true \
          --allow-gpu-metrics=true \
          --http-port=8080 \
          --grpc-port=8081 \
          --metrics-port=8002 \
          --http-thread-count=8 \
          --grpc-max-threads=8 \
          --log-verbose=1 \
          --disable-auto-complete-config
      ports:
      - containerPort: 8080
        name: http
        protocol: TCP
      - containerPort: 8081
        name: grpc
        protocol: TCP
      - containerPort: 8002
        name: metrics
        protocol: TCP
      env:
      - name: STORAGE_URI
        value: /mnt/models
      resources:
        requests:
          cpu: 500m
          memory: 1Gi
        limits:
          cpu: "2"
          memory: 4Gi
      volumeMounts:
      - name: model-dir
        mountPath: /mnt/models
      livenessProbe:
        httpGet:
          path: /v2/health/live
          port: http
        initialDelaySeconds: 30
        periodSeconds: 30
        timeoutSeconds: 10
      readinessProbe:
        httpGet:
          path: /v2/health/ready
          port: http
        initialDelaySeconds: 10
        periodSeconds: 10
        timeoutSeconds: 5
    multiModel: true
    supportedModelFormats:
    - name: tensorflow
      version: "1"
      autoSelect: true
    - name: tensorflow
      version: "2"
      autoSelect: true
    - name: pytorch
      version: "1"
      autoSelect: true
    - name: onnx
      version: "1"
      autoSelect: true
    - name: tensorrt
      version: "8"
      autoSelect: true
    - name: python
      version: "1"
      autoSelect: true
    volumes:
    - name: model-dir
      emptyDir: {}
