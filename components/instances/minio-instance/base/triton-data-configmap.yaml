apiVersion: v1
kind: ConfigMap
metadata:
  name: triton-data-configmap
  namespace: minio
  labels:
    app.kubernetes.io/name: triton-data-configmap
    app.kubernetes.io/part-of: triton-demo
data:
  # Script de prÃ©paration des donnÃ©es
  data_preprocessing.py: |
    import os
    import pickle
    import numpy as np
    from sklearn.datasets import load_iris
    from sklearn.model_selection import train_test_split

    print("ğŸ”§ PrÃ©paration des donnÃ©es Iris")
    print("=" * 40)

    # Configuration des chemins S3
    PROJECT_NAME = os.getenv('PROJECT_NAME', 'iris')
    S3_DATA_PATH = f"{PROJECT_NAME}-data"

    # Charger et prÃ©parer les donnÃ©es
    iris = load_iris()
    X, y = iris.data, iris.target
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=float(os.getenv('TEST_SIZE', 0.2)), 
        random_state=int(os.getenv('RANDOM_STATE', 42))
    )

    # Sauvegarder les donnÃ©es (sera uploadÃ© vers S3/triton-data/iris-data/)
    with open('X_train.pkl', 'wb') as f:
        pickle.dump(X_train, f)
    with open('X_test.pkl', 'wb') as f:
        pickle.dump(X_test, f)
    with open('y_train.pkl', 'wb') as f:
        pickle.dump(y_train, f)
    with open('y_test.pkl', 'wb') as f:
        pickle.dump(y_test, f)

    print(f"âœ… DonnÃ©es prÃ©parÃ©es: {X_train.shape[0]} train, {X_test.shape[0]} test")
    print(f"ğŸ“Š Features: {X_train.shape[1]}")
    print(f"ğŸ¯ Classes: {len(np.unique(y))}")
    print(f"ğŸ“ S3 Path: triton-data/{S3_DATA_PATH}/")

  # Script d'entraÃ®nement du modÃ¨le
  model_training.py: |
    import os
    import pickle
    from sklearn.ensemble import RandomForestClassifier

    print("ğŸ¤– EntraÃ®nement du modÃ¨le Random Forest")
    print("=" * 40)

    # Configuration des chemins S3
    PROJECT_NAME = os.getenv('PROJECT_NAME', 'iris')
    S3_MODELS_PATH = f"{PROJECT_NAME}-models"

    # Charger les donnÃ©es
    with open('X_train.pkl', 'rb') as f:
        X_train = pickle.load(f)
    with open('y_train.pkl', 'rb') as f:
        y_train = pickle.load(f)

    # EntraÃ®ner le modÃ¨le
    model = RandomForestClassifier(
        n_estimators=int(os.getenv('N_ESTIMATORS', 100)),
        random_state=int(os.getenv('RANDOM_STATE', 42))
    )
    model.fit(X_train, y_train)

    # Sauvegarder le modÃ¨le (sera uploadÃ© vers Model Registry via "Triton Demo - S3 Connection")
    with open('iris_model.pkl', 'wb') as f:
        pickle.dump(model, f)

    print("âœ… ModÃ¨le entraÃ®nÃ© et sauvegardÃ©")
    print(f"ğŸŒ³ Nombre d'arbres: {model.n_estimators}")
    print(f"ğŸ“ S3 Path: [Model Registry]/{S3_MODELS_PATH}/")

  # Script d'Ã©valuation du modÃ¨le
  model_registry.py: |
    import os
    import pickle
    import json
    from sklearn.metrics import accuracy_score, classification_report

    print("ğŸ“Š Ã‰valuation du modÃ¨le")
    print("=" * 30)

    # Configuration des chemins S3
    PROJECT_NAME = os.getenv('PROJECT_NAME', 'iris')
    S3_MODELS_PATH = f"{PROJECT_NAME}-models"

    # Charger le modÃ¨le et les donnÃ©es
    with open('iris_model.pkl', 'rb') as f:
        model = pickle.load(f)
    with open('X_test.pkl', 'rb') as f:
        X_test = pickle.load(f)
    with open('y_test.pkl', 'rb') as f:
        y_test = pickle.load(f)

    # Ã‰valuation
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)

    # Sauvegarder les mÃ©triques (sera uploadÃ© vers Model Registry via "Triton Demo - S3 Connection")
    metrics = {
        'accuracy': accuracy,
        'classification_report': classification_report(y_test, y_pred, output_dict=True)
    }

    with open('metrics.pkl', 'wb') as f:
        pickle.dump(metrics, f)

    with open('accuracy.txt', 'w') as f:
        f.write(f"Accuracy: {accuracy:.4f}")

    print(f"âœ… MÃ©triques sauvegardÃ©es - Accuracy: {accuracy:.4f}")
    print("ğŸ“‹ Rapport de classification:")
    print(classification_report(y_test, y_pred))
    print(f"ğŸ“ S3 Path: [Model Registry]/{S3_MODELS_PATH}/")

  # Configuration des data connections
  data_connections.md: |
    # Configuration des Data Connections

    ## Data Connection pour les donnÃ©es:
    - **Nom**: `triton-data-connection`
    - **Type**: `S3`
    - **Endpoint**: `http://minio-api.minio.svc:9000`
    - **Bucket**: `triton-data`
    - **Usage**: Stockage des donnÃ©es d'entraÃ®nement et de test

    ## Data Connection pour les modÃ¨les (existante):
    - **Nom**: `Triton Demo - S3 Connection`
    - **Type**: `S3`
    - **Usage**: Model Registry - stockage des modÃ¨les entraÃ®nÃ©s

    ## Structure des donnÃ©es pour Iris:
    ```
    triton-data/
    â””â”€â”€ iris-data/
        â”œâ”€â”€ X_train.pkl
        â”œâ”€â”€ X_test.pkl
        â”œâ”€â”€ y_train.pkl
        â””â”€â”€ y_test.pkl

    [Model Registry Bucket]/
    â””â”€â”€ iris-models/
        â”œâ”€â”€ iris_model.pkl
        â”œâ”€â”€ metrics.pkl
        â””â”€â”€ accuracy.txt
    ```

    ## Configuration dans Elyra:
    1. **NÅ“ud data_preprocessing**: Utilise `triton-data-connection`
    2. **NÅ“ud model_training**: Utilise `triton-data-connection` pour les donnÃ©es
    3. **NÅ“ud model_registry**: Utilise `Triton Demo - S3 Connection` pour les modÃ¨les

    ## Utilisation dans Elyra:
    1. Configurez la data connection avec les credentials S3
    2. Utilisez les buckets dans vos pipelines
    3. Les fichiers seront automatiquement uploadÃ©s/downloadÃ©s 