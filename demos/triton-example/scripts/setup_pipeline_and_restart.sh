#!/bin/bash

echo "ðŸš€ Configuration de la pipeline et redÃ©marrage du workbench"
echo "=========================================================="

# VÃ©rifier que nous sommes dans le bon rÃ©pertoire
if [ ! -f "pipelines/iris.pipeline" ]; then
    echo "âŒ Erreur: Veuillez exÃ©cuter ce script depuis le rÃ©pertoire triton-example"
    exit 1
fi

echo "ðŸ“‹ Pipeline complÃ©tÃ©e avec:"
echo "   - Data connections configurÃ©es"
echo "   - Outputs vers triton-data/iris-data/ et iris-models/"
echo "   - Variables d'environnement dÃ©finies"

# Copier les scripts de pipeline depuis le ConfigMap
echo "ðŸ“ Copie des scripts de pipeline..."
mkdir -p pipelines/

# CrÃ©er les scripts de pipeline
cat > pipelines/data_preprocessing.py << 'EOF'
import os
import pickle
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

print("ðŸ”§ PrÃ©paration des donnÃ©es Iris")
print("=" * 40)

# Configuration des chemins S3
PROJECT_NAME = os.getenv('PROJECT_NAME', 'iris')
S3_DATA_PATH = f"{PROJECT_NAME}-data"

# Charger et prÃ©parer les donnÃ©es
iris = load_iris()
X, y = iris.data, iris.target
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=float(os.getenv('TEST_SIZE', 0.2)),
    random_state=int(os.getenv('RANDOM_STATE', 42))
)

# Sauvegarder les donnÃ©es (sera uploadÃ© vers S3/triton-data/iris-data/)
with open('X_train.pkl', 'wb') as f:
    pickle.dump(X_train, f)
with open('X_test.pkl', 'wb') as f:
    pickle.dump(X_test, f)
with open('y_train.pkl', 'wb') as f:
    pickle.dump(y_train, f)
with open('y_test.pkl', 'wb') as f:
    pickle.dump(y_test, f)

print(f"âœ… DonnÃ©es prÃ©parÃ©es: {X_train.shape[0]} train, {X_test.shape[0]} test")
print(f"ðŸ“Š Features: {X_train.shape[1]}")
print(f"ðŸŽ¯ Classes: {len(np.unique(y))}")
print(f"ðŸ“ S3 Path: triton-data/{S3_DATA_PATH}/")
EOF

cat > pipelines/model_training.py << 'EOF'
import os
import pickle
from sklearn.ensemble import RandomForestClassifier

print("ðŸ¤– EntraÃ®nement du modÃ¨le Random Forest")
print("=" * 40)

# Configuration des chemins S3
PROJECT_NAME = os.getenv('PROJECT_NAME', 'iris')
S3_MODELS_PATH = f"{PROJECT_NAME}-models"

# Charger les donnÃ©es
with open('X_train.pkl', 'rb') as f:
    X_train = pickle.load(f)
with open('y_train.pkl', 'rb') as f:
    y_train = pickle.load(f)

# EntraÃ®ner le modÃ¨le
model = RandomForestClassifier(
    n_estimators=int(os.getenv('N_ESTIMATORS', 100)),
    random_state=int(os.getenv('RANDOM_STATE', 42))
)
model.fit(X_train, y_train)

# Sauvegarder le modÃ¨le (sera uploadÃ© vers Model Registry via "Triton Demo - S3 Connection")
with open('iris_model.pkl', 'wb') as f:
    pickle.dump(model, f)

print("âœ… ModÃ¨le entraÃ®nÃ© et sauvegardÃ©")
print(f"ðŸŒ³ Nombre d'arbres: {model.n_estimators}")
print(f"ðŸ“ S3 Path: [Model Registry]/{S3_MODELS_PATH}/")
EOF

cat > pipelines/model_registry.py << 'EOF'
import os
import pickle
import json
from sklearn.metrics import accuracy_score, classification_report

print("ðŸ“Š Ã‰valuation du modÃ¨le")
print("=" * 30)

# Configuration des chemins S3
PROJECT_NAME = os.getenv('PROJECT_NAME', 'iris')
S3_MODELS_PATH = f"{PROJECT_NAME}-models"

# Charger le modÃ¨le et les donnÃ©es
with open('iris_model.pkl', 'rb') as f:
    model = pickle.load(f)
with open('X_test.pkl', 'rb') as f:
    X_test = pickle.load(f)
with open('y_test.pkl', 'rb') as f:
    y_test = pickle.load(f)

# Ã‰valuation
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

# Sauvegarder les mÃ©triques (sera uploadÃ© vers Model Registry via "Triton Demo - S3 Connection")
metrics = {
    'accuracy': accuracy,
    'classification_report': classification_report(y_test, y_pred, output_dict=True)
}

with open('metrics.pkl', 'wb') as f:
    pickle.dump(metrics, f)

with open('accuracy.txt', 'w') as f:
    f.write(f"Accuracy: {accuracy:.4f}")

print(f"âœ… MÃ©triques sauvegardÃ©es - Accuracy: {accuracy:.4f}")
print("ðŸ“‹ Rapport de classification:")
print(classification_report(y_test, y_pred))
print(f"ðŸ“ S3 Path: [Model Registry]/{S3_MODELS_PATH}/")
EOF

echo "âœ… Scripts de pipeline crÃ©Ã©s"

# RedÃ©marrer le workbench
echo "ðŸ”„ RedÃ©marrage du workbench..."
oc delete pod -n triton-demo -l app.kubernetes.io/name=triton-workbench --force --grace-period=0

echo "â³ Attente du redÃ©marrage du workbench..."
sleep 10

# VÃ©rifier le statut
echo "ðŸ” VÃ©rification du statut du workbench..."
oc get pods -n triton-demo -l app.kubernetes.io/name=triton-workbench

echo ""
echo "ðŸŽ‰ Configuration terminÃ©e !"
echo ""
echo "ðŸ“‹ Prochaines Ã©tapes dans Elyra:"
echo "1. Ouvrez le workbench JupyterLab"
echo "2. Allez dans Elyra â†’ Pipelines"
echo "3. Ouvrez la pipeline 'iris.pipeline'"
echo "4. Configurez les data connections:"
echo "   - NÅ“ud data_preprocessing: triton-data-connection"
echo "   - NÅ“ud model_training: triton-data-connection"
echo "   - NÅ“ud model_registry: Triton Demo - S3 Connection"
echo "5. Lancez la pipeline !"
echo ""
echo "ðŸ“ Fichiers crÃ©Ã©s:"
echo "   - pipelines/iris.pipeline (pipeline complÃ©tÃ©e)"
echo "   - pipelines/data_preprocessing.py"
echo "   - pipelines/model_training.py"
echo "   - pipelines/model_registry.py" 