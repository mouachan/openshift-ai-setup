# Triton Inference Server Demo

Cette d√©mo illustre un workflow complet d'entra√Ænement et de d√©ploiement d'un mod√®le avec NVIDIA Triton Inference Server dans OpenShift AI.

## Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Data Science  ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Elyra Pipeline  ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Model Registry  ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Triton Serving  ‚îÇ
‚îÇ    Workbench    ‚îÇ    ‚îÇ   (Kubeflow)     ‚îÇ    ‚îÇ   (MySQL+S3)    ‚îÇ    ‚îÇ   (KServe)       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Pipeline Kubeflow (3 √©tapes)

1. **Data Transformation** (`data_preprocessing.py`)
   - Chargement et nettoyage des donn√©es
   - Feature engineering
   - Division train/test

2. **Model Training** (`model_training.py`) 
   - Entra√Ænement d'un mod√®le simple (scikit-learn)
   - Validation du mod√®le
   - Export au format TensorFlow SavedModel pour Triton

3. **Model Registry** (`model_registry.py`)
   - Push du mod√®le vers le Model Registry
   - M√©tadonn√©es et versioning
   - Int√©gration avec MinIO S3

## D√©ploiement

- **GitOps** : D√©ploiement automatique via ArgoCD
- **KServe** : Single Model Serving avec Triton runtime
- **Inference** : Script HTTP pour tester l'inf√©rence

## Structure

```
demos/triton-example/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ pipelines/
‚îÇ   ‚îú‚îÄ‚îÄ iris_classification_pipeline.py    # Pipeline Elyra/Kubeflow
‚îÇ   ‚îú‚îÄ‚îÄ data_preprocessing.py              # √âtape 1: Transformation
‚îÇ   ‚îú‚îÄ‚îÄ model_training.py                  # √âtape 2: Entra√Ænement  
‚îÇ   ‚îî‚îÄ‚îÄ model_registry.py                  # √âtape 3: Registry
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ iris_classification_notebook.ipynb # Notebook de d√©veloppement
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îî‚îÄ‚îÄ iris_model/                        # Mod√®le export√© pour Triton
‚îî‚îÄ‚îÄ scripts/
    ‚îî‚îÄ‚îÄ test_inference.py                  # Test d'inf√©rence HTTP
```

> **Note** : Le r√©pertoire `deployment/` a √©t√© supprim√© car le d√©ploiement se fait maintenant via le GitOps int√©gr√©.
```

## ÔøΩ Ex√©cution de la d√©mo

### Option 1: Pipeline automatis√© (Kubeflow)
```bash
# 1. Lancer le pipeline Kubeflow complet
python pipelines/iris_classification_pipeline.py

# 2. Surveiller l'ex√©cution dans l'interface Kubeflow
# URL: https://your-cluster/pipelines
```

### Option 2: Notebook interactif (Workbench)
```bash
# 1. Acc√©der au workbench via OpenShift AI Dashboard
# URL: https://rhods-dashboard-redhat-ods-applications.apps.cluster.local/projects/triton-demo

# 2. Le workbench clone automatiquement la d√©mo Triton depuis GitHub
# 3. Ouvrir triton-demo/notebooks/iris_classification_notebook.ipynb
# 4. Ex√©cuter toutes les cellules
```

> **Note** : Le workbench clone automatiquement la d√©mo Triton depuis GitHub au d√©marrage.

### Option 3: Ex√©cution manuelle des √©tapes
```bash
# 1. Pr√©paration des donn√©es
python pipelines/data_preprocessing.py

# 2. Entra√Ænement du mod√®le
python pipelines/model_training.py

# 3. Enregistrement dans Model Registry
python pipelines/model_registry.py
```

## üöÄ D√©ploiement du mod√®le

### D√©ploiement automatique (GitOps int√©gr√©)
```bash
# Le GitOps est maintenant int√©gr√© dans le GitOps principal
# La d√©mo se d√©ploie automatiquement avec OpenShift AI

# V√©rifier le statut du GitOps int√©gr√©
make check-gitops

# V√©rification du statut
./scripts/deploy.sh status

# Test d'inf√©rence
./scripts/deploy.sh test
```

### D√©ploiement manuel (d√©ploiement du mod√®le uniquement)
```bash
# 1. Appliquer les configurations Kustomize
oc apply -k ../../components/instances/triton-demo-instance/base/model-serving/ -n triton-demo

# 2. Attendre que le service soit pr√™t
oc wait --for=condition=Ready inferenceservice/iris-classifier-triton -n triton-demo --timeout=300s

# 3. R√©cup√©rer l'URL du service
oc get inferenceservice iris-classifier-triton -n triton-demo -o jsonpath='{.status.url}'
```

## üß™ Tests d'inf√©rence

### Test automatique
```bash
# Test complet avec script Python
python scripts/test_inference.py --url <service-url>

# Test avec donn√©es personnalis√©es
python scripts/test_inference.py --url <service-url> --custom-data "[[5.1,3.5,1.4,0.2]]"
```

## üîÑ Migration vers GitOps int√©gr√©

### ‚ö†Ô∏è Changements importants
La d√©mo Triton a √©t√© migr√©e vers le **GitOps int√©gr√©** dans le GitOps principal d'OpenShift AI.

### ‚úÖ Avantages de la migration
- **Configuration unifi√©e** : Un seul GitOps pour tout
- **D√©ploiement automatique** : La d√©mo se d√©ploie avec l'infrastructure
- **Maintenance simplifi√©e** : Une seule configuration √† g√©rer
- **Coh√©rence garantie** : Utilise l'infrastructure d√©ploy√©e

### üìö Documentation de migration
- **Documentation compl√®te** : `../../docs/TRITON-DEMO-GITOPS-MIGRATION.md`
- **Script de migration** : `../../scripts/migrate-triton-demo-to-gitops.sh`
- **Composant int√©gr√©** : `../../components/instances/triton-demo-instance/`

### üöÄ Utilisation du nouveau GitOps
```bash
# D√©ploiement automatique avec OpenShift AI
oc apply -f ../../argocd-apps/openshift-ai-application.yaml

# V√©rification du statut
make check-gitops

# Acc√®s aux services
oc get all -n triton-demo
```

### Test manuel avec curl
```bash
# Test de sant√©
curl -X GET <service-url>/v2/health/ready

# Test d'inf√©rence
curl -X POST <service-url>/v2/models/iris_classifier/versions/1/infer 
  -H "Content-Type: application/json" 
  -d '{
    "inputs": [
      {
        "name": "input_features",
        "shape": [1, 4],
        "datatype": "FP32",
        "data": [5.1, 3.5, 1.4, 0.2]
      }
    ],
    "outputs": [
      {"name": "predictions"},
      {"name": "probabilities"}
    ]
  }'
```
