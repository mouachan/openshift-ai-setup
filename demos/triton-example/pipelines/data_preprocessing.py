#!/usr/bin/env python3
"""
Ã‰tape 1: Data Preprocessing
Transformation et prÃ©paration des donnÃ©es Iris pour l'entraÃ®nement
"""

import pandas as pd
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import os
import pickle
import argparse

def preprocess_data(output_path: str = "/tmp/data"):
    """PrÃ©paration des donnÃ©es Iris"""
    
    print("ğŸ”„ Chargement des donnÃ©es Iris...")
    
    # Charger le dataset Iris
    iris = load_iris()
    X, y = iris.data, iris.target
    feature_names = iris.feature_names
    target_names = iris.target_names
    
    print(f"ğŸ“Š Dataset: {X.shape[0]} Ã©chantillons, {X.shape[1]} features")
    print(f"ğŸ“Š Classes: {list(target_names)}")
    
    # CrÃ©ation du DataFrame
    df = pd.DataFrame(X, columns=feature_names)
    df['target'] = y
    df['species'] = [target_names[i] for i in y]
    
    print("ğŸ”„ Division train/test (80/20)...")
    
    # Division train/test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print("ğŸ”„ Normalisation des features...")
    
    # Normalisation
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # CrÃ©er le rÃ©pertoire de sortie
    os.makedirs(output_path, exist_ok=True)
    
    print(f"ğŸ’¾ Sauvegarde des donnÃ©es dans {output_path}...")
    
    # Sauvegarder les donnÃ©es preprocessÃ©es
    np.save(f"{output_path}/X_train.npy", X_train_scaled)
    np.save(f"{output_path}/X_test.npy", X_test_scaled)
    np.save(f"{output_path}/y_train.npy", y_train)
    np.save(f"{output_path}/y_test.npy", y_test)
    
    # Sauvegarder le scaler pour l'infÃ©rence
    with open(f"{output_path}/scaler.pkl", "wb") as f:
        pickle.dump(scaler, f)
    
    # Sauvegarder les mÃ©tadonnÃ©es
    metadata = {
        "feature_names": feature_names.tolist(),
        "target_names": target_names.tolist(),
        "n_features": X.shape[1],
        "n_classes": len(target_names),
        "train_size": len(X_train),
        "test_size": len(X_test)
    }
    
    with open(f"{output_path}/metadata.pkl", "wb") as f:
        pickle.dump(metadata, f)
    
    print("âœ… Preprocessing terminÃ© avec succÃ¨s!")
    print(f"ğŸ“ˆ Train set: {X_train_scaled.shape}")
    print(f"ğŸ“ˆ Test set: {X_test_scaled.shape}")
    
    return output_path

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--output-path", default="/tmp/data", help="Chemin de sortie des donnÃ©es")
    args = parser.parse_args()
    
    preprocess_data(args.output_path)
