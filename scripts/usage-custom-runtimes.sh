#!/bin/bash
# Guide d'utilisation des Custom Serving Runtimes
# Apr√®s d√©ploiement des templates Triton et Seldon

echo "üéØ Custom Serving Runtimes - Guide d'Utilisation"
echo "================================================"

# Couleurs
GREEN='\033[0;32m'
BLUE='\033[0;34m'
YELLOW='\033[1;33m'
NC='\033[0m'

echo ""
echo -e "${BLUE}üìã Runtimes D√©ploy√©s${NC}"
echo "======================"

# V√©rifier les templates
TRITON_TEMPLATE=$(oc get template triton-runtime-template -n redhat-ods-applications --no-headers 2>/dev/null | awk '{print $1}')
SELDON_TEMPLATE=$(oc get template seldon-mlserver-runtime-template -n redhat-ods-applications --no-headers 2>/dev/null | awk '{print $1}')

if [ ! -z "$TRITON_TEMPLATE" ]; then
    echo -e "${GREEN}‚úÖ NVIDIA Triton Inference Server${NC}"
    echo "   - Formats: TensorFlow, PyTorch, ONNX, TensorRT, Python"
    echo "   - Template: $TRITON_TEMPLATE"
    echo ""
fi

if [ ! -z "$SELDON_TEMPLATE" ]; then
    echo -e "${GREEN}‚úÖ Seldon MLServer${NC}"  
    echo "   - Formats: scikit-learn, XGBoost, LightGBM, MLflow, Hugging Face"
    echo "   - Template: $SELDON_TEMPLATE"
    echo ""
fi

# V√©rifier les serving runtimes
echo -e "${BLUE}üöÄ Serving Runtimes Actifs${NC}"
echo "==========================="
oc get servingruntimes -n redhat-ods-applications --no-headers 2>/dev/null | while read line; do
    name=$(echo $line | awk '{print $1}')
    modeltype=$(echo $line | awk '{print $3}')
    echo -e "${GREEN}‚úÖ${NC} $name (type: $modeltype)"
done

echo ""
echo -e "${BLUE}üåê Acc√®s Interface OpenShift AI${NC}"
echo "==============================="
DASHBOARD_URL=$(oc get route rhods-dashboard -n redhat-ods-applications -o jsonpath='{.spec.host}' 2>/dev/null)
if [ ! -z "$DASHBOARD_URL" ]; then
    echo -e "üîó Dashboard: ${YELLOW}https://$DASHBOARD_URL${NC}"
    echo ""
    echo "üìç Pour voir les custom runtimes :"
    echo "   1. Connectez-vous au dashboard OpenShift AI"
    echo "   2. Allez dans Settings ‚Üí Serving Runtimes"
    echo "   3. Vous devriez voir :"
    echo "      ‚Ä¢ Triton Runtime 23.10"
    echo "      ‚Ä¢ Seldon MLServer Runtime" 
    echo ""
    echo "üìç Pour utiliser les runtimes :"
    echo "   1. Cr√©ez un Data Science Project"
    echo "   2. Configure Server ‚Üí Choisissez le runtime appropri√©"
    echo "   3. Deploy Model ‚Üí S√©lectionnez le format de mod√®le"
    echo ""
else
    echo -e "${YELLOW}‚ö†Ô∏è  Dashboard OpenShift AI non trouv√©${NC}"
fi

echo -e "${BLUE}üîß Validation${NC}"
echo "=============="
echo "Pour valider le d√©ploiement :"
echo "   ./scripts/test-custom-runtimes.sh"
echo ""

echo -e "${BLUE}üìö Documentation${NC}"  
echo "================="
echo "‚Ä¢ Triton: Chapitre 2.11.23 OpenShift AI 2.22"
echo "‚Ä¢ Seldon: Chapitre 2.11.3 OpenShift AI 2.22"
echo "‚Ä¢ R√©f√©rence: https://ai-on-openshift.io/odh-rhoai/custom-runtime-triton/"
echo ""

echo -e "${GREEN}üéâ Installation Termin√©e !${NC}"
echo "Les custom serving runtimes sont maintenant disponibles dans l'interface OpenShift AI."
