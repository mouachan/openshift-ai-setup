apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: llm-copy-daemonset
  namespace: default
  labels:
    app: llm-copy
spec:
  selector:
    matchLabels:
      app: llm-copy
  template:
    metadata:
      labels:
        app: llm-copy
    spec:
      containers:
      - name: llm-copy
        image: registry.redhat.io/ubi8/ubi:latest
        command: ["/bin/bash"]
        args:
        - -c
        - |
          echo "üöÄ Copie du LLM sur le n≈ìud $(hostname)..."
          
          # Cr√©er le r√©pertoire de destination
          mkdir -p /opt/llm/models
          
          # T√©l√©charger le mod√®le depuis une source
          cd /opt/llm/models
          
          # Exemple avec Hugging Face
          # git lfs install
          # git clone https://huggingface.co/meta-llama/Llama-2-7b-chat-hf
          
          # Ou depuis un serveur S3/MinIO
          # aws s3 cp s3://my-bucket/llama2-7b.tar.gz .
          # tar -xzf llama2-7b.tar.gz
          
          # Ou depuis un serveur HTTP
          # wget -O llama2-7b.tar.gz "https://example.com/models/llama2-7b.tar.gz"
          # tar -xzf llama2-7b.tar.gz
          
          echo "‚úÖ LLM copi√© sur $(hostname)"
          
          # Garder le Pod en vie pour v√©rification
          sleep 3600
        volumeMounts:
        - name: llm-storage
          mountPath: /opt/llm
        - name: host-storage
          mountPath: /host
          readOnly: true
      volumes:
      - name: llm-storage
        hostPath:
          path: /opt/llm
          type: DirectoryOrCreate
      - name: host-storage
        hostPath:
          path: /
          type: Directory
      restartPolicy: Always 